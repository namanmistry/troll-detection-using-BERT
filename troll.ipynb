{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install transformers ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv(\"/kaggle/input/troll-detection/troll.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:44:12.390008Z","iopub.execute_input":"2023-05-29T03:44:12.390413Z","iopub.status.idle":"2023-05-29T03:44:12.457362Z","shell.execute_reply.started":"2023-05-29T03:44:12.390385Z","shell.execute_reply":"2023-05-29T03:44:12.456571Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data.drop(columns=['Unnamed: 0'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:06:54.206805Z","iopub.execute_input":"2023-05-29T03:06:54.207074Z","iopub.status.idle":"2023-05-29T03:06:54.212579Z","shell.execute_reply.started":"2023-05-29T03:06:54.207051Z","shell.execute_reply":"2023-05-29T03:06:54.211847Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom tqdm import tqdm\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\ndef clean_text(raw_text):\n    # Remove unnecessary symbols and numbers\n    cleaned_text = re.sub('[^a-zA-Z]', ' ', raw_text)\n    \n    # Convert to lowercase\n    cleaned_text = cleaned_text.lower()\n    \n    # Tokenize the text\n    words = cleaned_text.split()\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n    \n    # Perform stemming\n    stemmer = PorterStemmer()\n    words = [stemmer.stem(word) for word in words]\n    \n    # Perform lemmatization\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n    \n    # Join the cleaned words back into a single string\n    cleaned_text = ' '.join(words)\n    \n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:45:07.926705Z","iopub.execute_input":"2023-05-29T03:45:07.927825Z","iopub.status.idle":"2023-05-29T03:45:07.940821Z","shell.execute_reply.started":"2023-05-29T03:45:07.927790Z","shell.execute_reply":"2023-05-29T03:45:07.939899Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"tqdm.pandas()\ndata['cleaned_text'] = data['tweet'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:06:55.086949Z","iopub.execute_input":"2023-05-29T03:06:55.087239Z","iopub.status.idle":"2023-05-29T03:07:06.887258Z","shell.execute_reply.started":"2023-05-29T03:06:55.087213Z","shell.execute_reply":"2023-05-29T03:07:06.886184Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\n\n# Set random seed for reproducibility\ntf.random.set_seed(42)\n    \n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# Define optimizer, loss function, and metrics\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n\n\n# Load the tokenizer and model\nwith tpu_strategy.scope():\n    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:07:06.889612Z","iopub.execute_input":"2023-05-29T03:07:06.889920Z","iopub.status.idle":"2023-05-29T03:08:02.176709Z","shell.execute_reply.started":"2023-05-29T03:07:06.889888Z","shell.execute_reply":"2023-05-29T03:08:02.175369Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"D0529 03:07:33.379958968    7177 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0529 03:07:33.379987196    7177 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0529 03:07:33.379990688    7177 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0529 03:07:33.379993486    7177 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0529 03:07:33.379995788    7177 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0529 03:07:33.379998148    7177 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0529 03:07:33.380001473    7177 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0529 03:07:33.380005618    7177 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0529 03:07:33.380008150    7177 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0529 03:07:33.380010554    7177 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0529 03:07:33.380012945    7177 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0529 03:07:33.380015330    7177 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0529 03:07:33.380017667    7177 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0529 03:07:33.380019998    7177 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0529 03:07:33.380210865    7177 ev_epoll1_linux.cc:122]               grpc epoll fd: 65\nD0529 03:07:33.380233875    7177 ev_posix.cc:144]                      Using polling engine: epoll1\nD0529 03:07:33.380255186    7177 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0529 03:07:33.380685268    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0529 03:07:33.380694300    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0529 03:07:33.380700365    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0529 03:07:33.380703557    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0529 03:07:33.380706811    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0529 03:07:33.380710058    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0529 03:07:33.380716886    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0529 03:07:33.380732743    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0529 03:07:33.380762485    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0529 03:07:33.380780303    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0529 03:07:33.380784086    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0529 03:07:33.380787284    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0529 03:07:33.380790938    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0529 03:07:33.380794301    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0529 03:07:33.380797776    7177 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0529 03:07:33.380803402    7177 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0529 03:07:33.383374129    7177 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0529 03:07:33.419151534    7397 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0529 03:07:33.466689458    7177 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-05-29T03:07:33.466670712+00:00\", grpc_status:2}\n/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nAll model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define your training data\ntrain_texts = list(data['cleaned_text'].values)\ntrain_labels = list(data['class'].values) ","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:08:02.178008Z","iopub.execute_input":"2023-05-29T03:08:02.178342Z","iopub.status.idle":"2023-05-29T03:08:02.185518Z","shell.execute_reply.started":"2023-05-29T03:08:02.178315Z","shell.execute_reply":"2023-05-29T03:08:02.184514Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Tokenize the input texts\nfrom sklearn.model_selection import train_test_split\n# Split the data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n\n# Tokenize the input texts\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)\n\n# Create TensorFlow datasets for training and testing\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_labels\n)).shuffle(len(train_texts)).batch(8)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    test_labels\n)).batch(8)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:08:02.186747Z","iopub.execute_input":"2023-05-29T03:08:02.187046Z","iopub.status.idle":"2023-05-29T03:08:44.474127Z","shell.execute_reply.started":"2023-05-29T03:08:02.187019Z","shell.execute_reply":"2023-05-29T03:08:44.472749Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine-tune the model\nmodel.fit(train_dataset,validation_data=test_dataset, epochs=3, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:17:34.879147Z","iopub.execute_input":"2023-05-29T03:17:34.880208Z","iopub.status.idle":"2023-05-29T03:24:26.358293Z","shell.execute_reply.started":"2023-05-29T03:17:34.880171Z","shell.execute_reply":"2023-05-29T03:24:26.356948Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/3\n2000/2000 [==============================] - 137s 68ms/step - loss: 0.1295 - accuracy: 0.9541 - val_loss: 0.3367 - val_accuracy: 0.8908\nEpoch 2/3\n2000/2000 [==============================] - 136s 68ms/step - loss: 0.0866 - accuracy: 0.9693 - val_loss: 0.3245 - val_accuracy: 0.9108\nEpoch 3/3\n2000/2000 [==============================] - 137s 68ms/step - loss: 0.0688 - accuracy: 0.9761 - val_loss: 0.4835 - val_accuracy: 0.8848\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f81b80eaac0>"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"troll_6_epoch.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:24:35.573245Z","iopub.execute_input":"2023-05-29T03:24:35.573672Z","iopub.status.idle":"2023-05-29T03:24:37.118104Z","shell.execute_reply.started":"2023-05-29T03:24:35.573639Z","shell.execute_reply":"2023-05-29T03:24:37.116643Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\ndef predict_text_classification(texts, num_labels, model, tokenizer):\n  \n\n\n    # Tokenize the input texts\n    encoded_inputs = tokenizer.batch_encode_plus(\n        texts,\n        add_special_tokens=True,\n        truncation=True,\n        max_length=512,  # Adjust this value based on your model's maximum input length\n        padding=\"max_length\",\n        return_attention_mask=True,\n        return_tensors=\"tf\"\n    )\n\n    # Perform the prediction\n    logits = model.predict(encoded_inputs)[0]\n    probabilities = tf.nn.softmax(logits, axis=1)\n    predicted_labels = tf.argmax(probabilities, axis=1).numpy()\n\n    return predicted_labels.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:29:58.119302Z","iopub.execute_input":"2023-05-29T03:29:58.120023Z","iopub.status.idle":"2023-05-29T03:29:58.127377Z","shell.execute_reply.started":"2023-05-29T03:29:58.119992Z","shell.execute_reply":"2023-05-29T03:29:58.126007Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"texts = [\"This is the first text.\", \"This is the second text.\"]\nnum_labels = 2\npredicted_labels = predict_text_classification(texts, num_labels, model, tokenizer)\nprint(predicted_labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:30:44.506824Z","iopub.execute_input":"2023-05-29T03:30:44.507655Z","iopub.status.idle":"2023-05-29T03:30:48.793696Z","shell.execute_reply.started":"2023-05-29T03:30:44.507622Z","shell.execute_reply":"2023-05-29T03:30:48.792261Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the first text.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the second text.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 3\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_text_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_labels)\n","Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mpredict_text_classification\u001b[0;34m(texts, num_labels, model, tokenizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m      7\u001b[0m     texts,\n\u001b[1;32m      8\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform the prediction\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(probabilities, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_cache.py:79\u001b[0m, in \u001b[0;36mFunctionCache.add\u001b[0;34m(self, context, function_type, deletion_observer, concrete_fn)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\u001b[38;5;28mself\u001b[39m, context: FunctionContext,\n\u001b[1;32m     68\u001b[0m         function_type: function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType,\n\u001b[1;32m     69\u001b[0m         deletion_observer: trace_type\u001b[38;5;241m.\u001b[39mWeakrefDeletionObserver,\n\u001b[1;32m     70\u001b[0m         concrete_fn: Any):\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Adds a new concrete function alongside its key.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    concrete_fn: The concrete function to be added to the cache.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_primary\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m concrete_fn\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_dict:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_dict[context] \u001b[38;5;241m=\u001b[39m type_dispatch\u001b[38;5;241m.\u001b[39mTypeDispatchTable()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py:314\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 314\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py:144\u001b[0m, in \u001b[0;36mParameter.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Cannot generate a hashable key for DistributedIteratorSpec(((('/job:localhost/replica:0/task:0/device:CPU:0', ['/job:localhost/replica:0/task:0/device:TPU:0', '/job:localhost/replica:0/task:0/device:TPU:1', '/job:localhost/replica:0/task:0/device:TPU:2', '/job:localhost/replica:0/task:0/device:TPU:3', '/job:localhost/replica:0/task:0/device:TPU:4', '/job:localhost/replica:0/task:0/device:TPU:5', '/job:localhost/replica:0/task:0/device:TPU:6', '/job:localhost/replica:0/task:0/device:TPU:7']),), True), {'input_ids': PerReplicaSpec(TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None)), 'token_type_ids': PerReplicaSpec(TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None)), 'attention_mask': PerReplicaSpec(TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None))}, 140248647556928, 140254846210592) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>"],"ename":"ValueError","evalue":"Cannot generate a hashable key for DistributedIteratorSpec(((('/job:localhost/replica:0/task:0/device:CPU:0', ['/job:localhost/replica:0/task:0/device:TPU:0', '/job:localhost/replica:0/task:0/device:TPU:1', '/job:localhost/replica:0/task:0/device:TPU:2', '/job:localhost/replica:0/task:0/device:TPU:3', '/job:localhost/replica:0/task:0/device:TPU:4', '/job:localhost/replica:0/task:0/device:TPU:5', '/job:localhost/replica:0/task:0/device:TPU:6', '/job:localhost/replica:0/task:0/device:TPU:7']),), True), {'input_ids': PerReplicaSpec(TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None)), 'token_type_ids': PerReplicaSpec(TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None)), 'attention_mask': PerReplicaSpec(TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None))}, 140248647556928, 140254846210592) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>","output_type":"error"}]},{"cell_type":"code","source":"valid","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:28:44.315070Z","iopub.execute_input":"2023-05-29T03:28:44.315805Z","iopub.status.idle":"2023-05-29T03:28:44.321530Z","shell.execute_reply.started":"2023-05-29T03:28:44.315768Z","shell.execute_reply":"2023-05-29T03:28:44.320443Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 2023, 2003, 3231, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Folder to be zipped\nfolder_path = '/kaggle/working/troll_6_epoch.h5'\n\n# Zip file path\nzip_path = '/kaggle/working/troll_6_epoch.zip'\n\n# Zip the folder\nshutil.make_archive(zip_path, 'zip', folder_path)\n\n# Download the zip file\nos.chdir('/kaggle/working')\nprint('Downloading the zip file...')\nfrom IPython.display import FileLink\nFileLink(zip_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:32:10.144869Z","iopub.execute_input":"2023-05-29T03:32:10.145758Z","iopub.status.idle":"2023-05-29T03:32:35.866177Z","shell.execute_reply.started":"2023-05-29T03:32:10.145725Z","shell.execute_reply":"2023-05-29T03:32:35.864863Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Downloading the zip file...\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/troll_6_epoch.zip","text/html":"Path (<tt>/kaggle/working/troll_6_epoch.zip</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(\"troll_6_epoch.zip.zip\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:39:45.368643Z","iopub.execute_input":"2023-05-29T03:39:45.368946Z","iopub.status.idle":"2023-05-29T03:39:45.375972Z","shell.execute_reply.started":"2023-05-29T03:39:45.368925Z","shell.execute_reply":"2023-05-29T03:39:45.374755Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/troll_6_epoch.zip.zip","text/html":"<a href='troll_6_epoch.zip.zip' target='_blank'>troll_6_epoch.zip.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertForSequenceClassification.from_pretrained('/kaggle/working/troll_6_epoch.h5', num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:41:49.614657Z","iopub.execute_input":"2023-05-29T03:41:49.614965Z","iopub.status.idle":"2023-05-29T03:41:53.007412Z","shell.execute_reply.started":"2023-05-29T03:41:49.614943Z","shell.execute_reply":"2023-05-29T03:41:53.006216Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad296dcc954f4b948ed66297c1898795"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"948c8729134942a691cc2a0564433359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf7452b9fab4241a18d7ed887fba281"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/working/troll_6_epoch.h5 were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /kaggle/working/troll_6_epoch.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence1 = \" Get fucking real dude.\"\n\n\n# Tokenize and encode the sentences\ninput_ids = tokenizer.encode_plus(\n    sentence1,\n    add_special_tokens=True,\n    max_length=128,\n    padding='longest',\n    truncation=True,\n    return_tensors='tf'\n)['input_ids']\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:46:09.965247Z","iopub.execute_input":"2023-05-29T03:46:09.965675Z","iopub.status.idle":"2023-05-29T03:46:09.975080Z","shell.execute_reply.started":"2023-05-29T03:46:09.965646Z","shell.execute_reply":"2023-05-29T03:46:09.973853Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Make predictions\npredictions = model.predict(input_ids)[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:46:11.430179Z","iopub.execute_input":"2023-05-29T03:46:11.431152Z","iopub.status.idle":"2023-05-29T03:46:11.560251Z","shell.execute_reply.started":"2023-05-29T03:46:11.431123Z","shell.execute_reply":"2023-05-29T03:46:11.559076Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 69ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:46:13.660764Z","iopub.execute_input":"2023-05-29T03:46:13.661250Z","iopub.status.idle":"2023-05-29T03:46:13.671066Z","shell.execute_reply.started":"2023-05-29T03:46:13.661212Z","shell.execute_reply":"2023-05-29T03:46:13.670062Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([[-1.9141605,  1.6880792]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# Get predicted label and confidence scores\npredicted_label = tf.argmax(predictions, axis=1).numpy()[0]\nconfidence_scores = tf.nn.softmax(predictions, axis=1).numpy()[0]\n\n# Interpret the predicted label\nif predicted_label == 0:\n    print(\"Sentence belongs to class 0.\")\nelse:\n    print(\"Sentence belongs to class 1.\")\n\n# Print confidence scores for each class\nprint(\"Confidence scores:\", confidence_scores)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T03:46:19.957013Z","iopub.execute_input":"2023-05-29T03:46:19.957329Z","iopub.status.idle":"2023-05-29T03:46:19.965683Z","shell.execute_reply.started":"2023-05-29T03:46:19.957305Z","shell.execute_reply":"2023-05-29T03:46:19.964545Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Sentence belongs to class 1.\nConfidence scores: [0.02653907 0.973461  ]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}